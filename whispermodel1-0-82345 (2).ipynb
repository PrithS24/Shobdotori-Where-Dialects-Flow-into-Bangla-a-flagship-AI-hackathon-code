{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":119724,"databundleVersionId":14372637,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q \"protobuf==3.20.3\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:45:22.802234Z","iopub.execute_input":"2025-11-16T14:45:22.803322Z","iopub.status.idle":"2025-11-16T14:45:26.767044Z","shell.execute_reply.started":"2025-11-16T14:45:22.803294Z","shell.execute_reply":"2025-11-16T14:45:26.766125Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================\n# 0. Install dependencies\n# ============================================================\n!pip install -q transformers datasets soundfile librosa accelerate jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:40:53.418587Z","iopub.execute_input":"2025-11-16T14:40:53.418938Z","iopub.status.idle":"2025-11-16T14:42:29.018488Z","shell.execute_reply.started":"2025-11-16T14:40:53.418917Z","shell.execute_reply":"2025-11-16T14:42:29.017395Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# 1. Imports & paths\n# ============================================================\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport re\nimport random\n\nimport librosa\nimport torch\n\nfrom datasets import Dataset, Audio\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import (\n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n)\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\nDATA_ROOT = Path(\"/kaggle/input/shobdotori\")\nprint(\"Data root exists:\", DATA_ROOT.exists())\nprint(\"Subfolders:\", [p.name for p in DATA_ROOT.iterdir()])\n\n# Fix random seeds for some stability\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:46:04.730101Z","iopub.execute_input":"2025-11-16T14:46:04.731116Z","iopub.status.idle":"2025-11-16T14:46:04.742698Z","shell.execute_reply.started":"2025-11-16T14:46:04.731078Z","shell.execute_reply":"2025-11-16T14:46:04.741979Z"}},"outputs":[{"name":"stdout","text":"Data root exists: True\nSubfolders: ['sample_submission.csv', 'Train_annotation', 'Test', 'Train']\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# 2. Load & merge train annotations into a single DataFrame\n# ============================================================\nann_root = DATA_ROOT / \"Train_annotation\"\n\nall_ann_dfs = []\nfor csv_path in sorted(ann_root.glob(\"*.csv\")):\n    region = csv_path.stem  # e.g. \"Barisal\", \"Noakhali\"\n    df = pd.read_csv(csv_path)\n    df[\"region\"] = region\n    df[\"audio_path\"] = df[\"audio\"].apply(\n        lambda x: str(DATA_ROOT / \"Train\" / region / x)\n    )\n    all_ann_dfs.append(df)\n\ntrain_df = pd.concat(all_ann_dfs, ignore_index=True)\nprint(\"Train annotation shape:\", train_df.shape)\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:46:10.573152Z","iopub.execute_input":"2025-11-16T14:46:10.573436Z","iopub.status.idle":"2025-11-16T14:46:10.799032Z","shell.execute_reply.started":"2025-11-16T14:46:10.573415Z","shell.execute_reply":"2025-11-16T14:46:10.798349Z"}},"outputs":[{"name":"stdout","text":"Train annotation shape: (3350, 4)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                  audio                             text   region  \\\n0  female_barisal_1.wav  আজ সকালে আমি বাজারে গিয়েছিলাম।  Barisal   \n1  female_barisal_3.wav            আকাশে আজ মেঘ জমে আছে।  Barisal   \n2  female_barisal_4.wav   আমি বন্ধুদের সাথে খেলা খেলেছি।  Barisal   \n3  female_barisal_5.wav   দরজাটা ধীরে ধীরে বন্ধ করে দাও।  Barisal   \n4  female_barisal_6.wav    তুমি কি আমাকে পানি দিতে পারো?  Barisal   \n\n                                          audio_path  \n0  /kaggle/input/shobdotori/Train/Barisal/female_...  \n1  /kaggle/input/shobdotori/Train/Barisal/female_...  \n2  /kaggle/input/shobdotori/Train/Barisal/female_...  \n3  /kaggle/input/shobdotori/Train/Barisal/female_...  \n4  /kaggle/input/shobdotori/Train/Barisal/female_...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio</th>\n      <th>text</th>\n      <th>region</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>female_barisal_1.wav</td>\n      <td>আজ সকালে আমি বাজারে গিয়েছিলাম।</td>\n      <td>Barisal</td>\n      <td>/kaggle/input/shobdotori/Train/Barisal/female_...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>female_barisal_3.wav</td>\n      <td>আকাশে আজ মেঘ জমে আছে।</td>\n      <td>Barisal</td>\n      <td>/kaggle/input/shobdotori/Train/Barisal/female_...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>female_barisal_4.wav</td>\n      <td>আমি বন্ধুদের সাথে খেলা খেলেছি।</td>\n      <td>Barisal</td>\n      <td>/kaggle/input/shobdotori/Train/Barisal/female_...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>female_barisal_5.wav</td>\n      <td>দরজাটা ধীরে ধীরে বন্ধ করে দাও।</td>\n      <td>Barisal</td>\n      <td>/kaggle/input/shobdotori/Train/Barisal/female_...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>female_barisal_6.wav</td>\n      <td>তুমি কি আমাকে পানি দিতে পারো?</td>\n      <td>Barisal</td>\n      <td>/kaggle/input/shobdotori/Train/Barisal/female_...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ============================================================\n# 3. Simple Bangla text normalization (for labels & predictions)\n# ============================================================\n\ndef normalize_bn(text: str) -> str:\n    \"\"\"\n    Very simple Bangla normalization:\n    - strip leading/trailing whitespace\n    - collapse multiple spaces\n    - replace English '.' with Bangla '।'\n    You can extend this later if needed.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    text = text.strip()\n    text = text.replace(\".\", \"।\")\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text\n\n# Normalize labels in train_df (important for consistent training)\ntrain_df[\"text\"] = train_df[\"text\"].astype(str).apply(normalize_bn)\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:48:33.293103Z","iopub.execute_input":"2025-11-16T14:48:33.293448Z","iopub.status.idle":"2025-11-16T14:48:33.315115Z","shell.execute_reply.started":"2025-11-16T14:48:33.293425Z","shell.execute_reply":"2025-11-16T14:48:33.314414Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                  audio                             text   region  \\\n0  female_barisal_1.wav  আজ সকালে আমি বাজারে গিয়েছিলাম।  Barisal   \n1  female_barisal_3.wav            আকাশে আজ মেঘ জমে আছে।  Barisal   \n2  female_barisal_4.wav   আমি বন্ধুদের সাথে খেলা খেলেছি।  Barisal   \n3  female_barisal_5.wav   দরজাটা ধীরে ধীরে বন্ধ করে দাও।  Barisal   \n4  female_barisal_6.wav    তুমি কি আমাকে পানি দিতে পারো?  Barisal   \n\n                                          audio_path  \n0  /kaggle/input/shobdotori/Train/Barisal/female_...  \n1  /kaggle/input/shobdotori/Train/Barisal/female_...  \n2  /kaggle/input/shobdotori/Train/Barisal/female_...  \n3  /kaggle/input/shobdotori/Train/Barisal/female_...  \n4  /kaggle/input/shobdotori/Train/Barisal/female_...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio</th>\n      <th>text</th>\n      <th>region</th>\n      <th>audio_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>female_barisal_1.wav</td>\n      <td>আজ সকালে আমি বাজারে গিয়েছিলাম।</td>\n      <td>Barisal</td>\n      <td>/kaggle/input/shobdotori/Train/Barisal/female_...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>female_barisal_3.wav</td>\n      <td>আকাশে আজ মেঘ জমে আছে।</td>\n      <td>Barisal</td>\n      <td>/kaggle/input/shobdotori/Train/Barisal/female_...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>female_barisal_4.wav</td>\n      <td>আমি বন্ধুদের সাথে খেলা খেলেছি।</td>\n      <td>Barisal</td>\n      <td>/kaggle/input/shobdotori/Train/Barisal/female_...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>female_barisal_5.wav</td>\n      <td>দরজাটা ধীরে ধীরে বন্ধ করে দাও।</td>\n      <td>Barisal</td>\n      <td>/kaggle/input/shobdotori/Train/Barisal/female_...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>female_barisal_6.wav</td>\n      <td>তুমি কি আমাকে পানি দিতে পারো?</td>\n      <td>Barisal</td>\n      <td>/kaggle/input/shobdotori/Train/Barisal/female_...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Train / validation split (10% val, stratified by region)\ntrain_df_split, val_df_split = train_test_split(\n    train_df,\n    test_size=0.1,\n    random_state=SEED,\n    stratify=train_df[\"region\"]\n)\n\nprint(\"Train rows:\", len(train_df_split))\nprint(\"Val rows:\", len(val_df_split))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:55:56.010959Z","iopub.execute_input":"2025-11-16T18:55:56.011561Z","iopub.status.idle":"2025-11-16T18:55:56.022439Z","shell.execute_reply.started":"2025-11-16T18:55:56.011531Z","shell.execute_reply":"2025-11-16T18:55:56.021606Z"}},"outputs":[{"name":"stdout","text":"Train rows: 3015\nVal rows: 335\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"model_name = \"openai/whisper-medium\"  # ⬅️ change here\n\nprocessor = WhisperProcessor.from_pretrained(\n    model_name,\n    language=\"bengali\",\n    task=\"transcribe\"\n)\n\nmodel = WhisperForConditionalGeneration.from_pretrained(model_name)\nmodel.to(device)\n\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n    language=\"bengali\",\n    task=\"transcribe\"\n)\nmodel.config.suppress_tokens = []\nmodel.config.use_cache = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:55:59.514890Z","iopub.execute_input":"2025-11-16T18:55:59.515443Z","iopub.status.idle":"2025-11-16T18:56:12.210091Z","shell.execute_reply.started":"2025-11-16T18:55:59.515417Z","shell.execute_reply":"2025-11-16T18:56:12.209476Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b134ccfd99cb441f809c88d0fb935122"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de7cbb0fa5a2433ba456f12e9579c054"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"162046e960734cff9509a60713021e00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6514a7c7d0b42a2bf0c941d8a9c3d41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c733bdb4a1d84ac394696411934bb25a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18c422048ec14f7fa4185c7547b64a75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fb94b89d20b427187a920c62a78ef73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4b4fb3864414f23bebc4dfa02a8d825"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7effd25581041bbb0309e8269771d8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51ec0603e4984025871857a262f83506"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"562c1e7e9c504834960c11b9b5711fde"}},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# ============================================================\n# 6. Load Whisper-small processor & model\n# ============================================================\nmodel_name = \"openai/whisper-small\"\n\nprocessor = WhisperProcessor.from_pretrained(\n    model_name,\n    language=\"bengali\",\n    task=\"transcribe\"\n)\n\nmodel = WhisperForConditionalGeneration.from_pretrained(model_name)\nmodel.to(device)\n\n# Set decoding config for Bangla transcription\nmodel.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n    language=\"bengali\",\n    task=\"transcribe\"\n)\nmodel.config.suppress_tokens = []  # optional: don't suppress any tokens\nmodel.config.use_cache = False     # important for Trainer (avoids warnings)\n\nprocessor, model\n","metadata":{"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# # ============================================================\n# # 7. Preprocessing function: audio -> input_features, text -> labels\n# # ============================================================\n\n# import librosa\n\n# def prepare_dataset(batch):\n#     audio_path = batch[\"audio\"]          # string path\n#     audio_array, sr = librosa.load(audio_path, sr=16000)\n\n#     # 1) Input features (audio → log-mel)\n#     batch[\"input_features\"] = processor(\n#         audio_array,\n#         sampling_rate=sr\n#     ).input_features[0]\n\n#     # 2) Labels (text → token ids)  ✅ no as_target_processor\n#     text = batch[\"text\"]\n#     batch[\"labels\"] = processor.tokenizer(\n#         text,\n#         add_special_tokens=True\n#     ).input_ids\n\n#     return batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T05:40:09.164852Z","iopub.execute_input":"2025-11-16T05:40:09.165190Z","iopub.status.idle":"2025-11-16T05:40:09.170101Z","shell.execute_reply.started":"2025-11-16T05:40:09.165169Z","shell.execute_reply":"2025-11-16T05:40:09.169400Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# print(train_prepared[0].keys())\n# train_prepared[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T05:39:17.869191Z","iopub.status.idle":"2025-11-16T05:39:17.869392Z","shell.execute_reply.started":"2025-11-16T05:39:17.869295Z","shell.execute_reply":"2025-11-16T05:39:17.869304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader #0.82345\n\nclass ASRDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, processor: WhisperProcessor):\n        \"\"\"\n        df must contain: 'audio_path', 'text'\n        \"\"\"\n        self.df = df.reset_index(drop=True)\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        row = self.df.iloc[idx]\n        audio_path = row[\"audio_path\"]\n        text = row[\"text\"]\n\n        # 1) Load audio\n        audio_array, sr = librosa.load(audio_path, sr=16000)\n\n        # 2) Compute input features (log-mel spectrogram)\n        input_features = self.processor(\n            audio_array,\n            sampling_rate=16000\n        ).input_features[0]  # shape (80, T)\n\n        # 3) Tokenize labels\n        labels = self.processor.tokenizer(\n            text,\n            add_special_tokens=True\n        ).input_ids\n\n        return {\n            \"input_features\": input_features,\n            \"labels\": labels,\n        }\n\ntrain_dataset = ASRDataset(train_df_split, processor)\nval_dataset   = ASRDataset(val_df_split, processor)\n\nlen(train_dataset), len(val_dataset), train_dataset[0].keys()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:58:05.592851Z","iopub.execute_input":"2025-11-16T18:58:05.593488Z","iopub.status.idle":"2025-11-16T18:58:05.642703Z","shell.execute_reply.started":"2025-11-16T18:58:05.593463Z","shell.execute_reply":"2025-11-16T18:58:05.641937Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(3015, 335, dict_keys(['input_features', 'labels']))"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# from torch.utils.data import Dataset, DataLoader \n# class ASRDataset(Dataset):\n#     def __init__(self, df: pd.DataFrame, processor: WhisperProcessor, augment: bool = False):\n#         \"\"\"\n#         df must contain: 'audio_path', 'text'\n#         augment=True  -> apply speed & noise augmentation\n#         \"\"\"\n#         self.df = df.reset_index(drop=True)\n#         self.processor = processor\n#         self.augment = augment\n\n#     def __len__(self):\n#         return len(self.df)\n\n#     def __getitem__(self, idx: int):\n#         row = self.df.iloc[idx]\n#         audio_path = row[\"audio_path\"]\n#         text = row[\"text\"]\n\n#         audio_array, sr = librosa.load(audio_path, sr=16000)\n\n#         # --------- AUGMENTATION (train only) ----------\n#         if self.augment:\n#             # Random speed: 0.9x, 1.0x, 1.1x\n#             speed = random.choice([0.9, 1.0, 1.1])\n#             if speed != 1.0:\n#                 audio_array = librosa.effects.time_stretch(audio_array, rate=speed)\n\n#             # Small Gaussian noise\n#             noise_level = 0.003\n#             noise = np.random.randn(len(audio_array)) * noise_level\n#             audio_array = audio_array + noise\n#         # ------------------------------------------------\n\n#         input_features = self.processor(\n#             audio_array,\n#             sampling_rate=16000\n#         ).input_features[0]\n\n#         labels = self.processor.tokenizer(\n#             text,\n#             add_special_tokens=True\n#         ).input_ids\n\n#         return {\"input_features\": input_features, \"labels\": labels}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:00:10.852088Z","iopub.execute_input":"2025-11-16T15:00:10.852819Z","iopub.status.idle":"2025-11-16T15:00:10.859264Z","shell.execute_reply.started":"2025-11-16T15:00:10.852793Z","shell.execute_reply":"2025-11-16T15:00:10.858607Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# train_dataset = ASRDataset(train_df_split, processor, augment=True)\n# val_dataset   = ASRDataset(val_df_split,   processor, augment=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:00:18.184116Z","iopub.execute_input":"2025-11-16T15:00:18.184385Z","iopub.status.idle":"2025-11-16T15:00:18.189711Z","shell.execute_reply.started":"2025-11-16T15:00:18.184364Z","shell.execute_reply":"2025-11-16T15:00:18.189106Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Dict, List, Union\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: WhisperProcessor\n\n    def __call__(\n        self, features: List[Dict[str, Union[List[int], np.ndarray]]]\n    ) -> Dict[str, torch.Tensor]:\n        # Input features\n        input_features = [\n            {\"input_features\": f[\"input_features\"]} for f in features\n        ]\n        # Label features\n        label_features = [\n            {\"input_ids\": f[\"labels\"]} for f in features\n        ]\n\n        batch = self.processor.feature_extractor.pad(\n            input_features,\n            return_tensors=\"pt\",\n        )\n\n        labels_batch = self.processor.tokenizer.pad(\n            label_features,\n            return_tensors=\"pt\",\n        )\n\n        labels = labels_batch[\"input_ids\"].masked_fill(\n            labels_batch[\"attention_mask\"].ne(1), -100\n        )\n\n        batch[\"labels\"] = labels\n        return batch\n\ndata_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:58:13.690376Z","iopub.execute_input":"2025-11-16T18:58:13.690950Z","iopub.status.idle":"2025-11-16T18:58:13.696791Z","shell.execute_reply.started":"2025-11-16T18:58:13.690924Z","shell.execute_reply":"2025-11-16T18:58:13.696168Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# batch_size = 4  # if OOM: use 2; if lots of memory: try 8\nbatch_size = 2  # if OOM: drop to 1\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=data_collator,\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n    collate_fn=data_collator,\n)\n\nlen(train_loader), len(val_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:58:18.852726Z","iopub.execute_input":"2025-11-16T18:58:18.853413Z","iopub.status.idle":"2025-11-16T18:58:18.859242Z","shell.execute_reply.started":"2025-11-16T18:58:18.853390Z","shell.execute_reply":"2025-11-16T18:58:18.858464Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"(1508, 168)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"def levenshtein_distance(a: str, b: str) -> int:\n    n, m = len(a), len(b)\n    if n == 0:\n        return m\n    if m == 0:\n        return n\n\n    dp = [[0] * (m + 1) for _ in range(n + 1)]\n    for i in range(n + 1):\n        dp[i][0] = i\n    for j in range(m + 1):\n        dp[0][j] = j\n\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            cost = 0 if a[i - 1] == b[j - 1] else 1\n            dp[i][j] = min(\n                dp[i - 1][j] + 1,          # deletion\n                dp[i][j - 1] + 1,          # insertion\n                dp[i - 1][j - 1] + cost    # substitution\n            )\n    return dp[n][m]\n\ndef normalized_similarity(pred: str, ref: str) -> float:\n    pred = normalize_bn(pred)\n    ref = normalize_bn(ref)\n    if len(pred) == 0 and len(ref) == 0:\n        return 1.0\n    max_len = max(len(pred), len(ref))\n    if max_len == 0:\n        return 1.0\n    dist = levenshtein_distance(pred, ref)\n    sim = 1.0 - dist / max_len\n    return sim\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:58:23.117069Z","iopub.execute_input":"2025-11-16T18:58:23.117568Z","iopub.status.idle":"2025-11-16T18:58:23.124073Z","shell.execute_reply.started":"2025-11-16T18:58:23.117541Z","shell.execute_reply":"2025-11-16T18:58:23.123423Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def evaluate_model(model, val_loader, processor, device, max_length=128):\n    model.eval()\n    sims = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            input_features = batch[\"input_features\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            # Generate predictions\n            # generated_ids = model.generate(\n            #     input_features,\n            #     max_length=max_length\n            # )\n            generated_ids = model.generate(\n                input_features,\n                max_length=128,\n                num_beams=5,\n                early_stopping=True\n            )\n\n\n            # Decode predictions\n            pred_str = processor.tokenizer.batch_decode(\n                generated_ids,\n                skip_special_tokens=True\n            )\n\n            # Decode labels (replace -100 with pad_token_id)\n            labels_clone = labels.clone()\n            labels_clone[labels_clone == -100] = processor.tokenizer.pad_token_id\n            label_str = processor.tokenizer.batch_decode(\n                labels_clone,\n                skip_special_tokens=True\n            )\n\n            for p, r in zip(pred_str, label_str):\n                sims.append(normalized_similarity(p, r))\n\n    mean_sim = float(np.mean(sims)) if sims else 0.0\n    return mean_sim\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:58:28.440527Z","iopub.execute_input":"2025-11-16T18:58:28.441401Z","iopub.status.idle":"2025-11-16T18:58:28.447155Z","shell.execute_reply.started":"2025-11-16T18:58:28.441374Z","shell.execute_reply":"2025-11-16T18:58:28.446250Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"from torch.optim import AdamW\n\nlearning_rate = 1e-5\n# optimizer = AdamW(model.parameters(), lr=learning_rate)\noptimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n\n# num_epochs = 8  # start with 3; can increase later\nnum_epochs = 4  # or 4 max for a first run\n\nbest_val_sim = 0.0\nbest_model_path = \"./whisper-small-shobdotori-manual\"\n\nfor epoch in range(1, num_epochs + 1):\n    model.train()\n    total_loss = 0.0\n    step = 0\n\n    for batch in train_loader:\n        step += 1\n\n        input_features = batch[\"input_features\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(\n            input_features=input_features,\n            labels=labels\n        )\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        if step % 50 == 0:\n            print(f\"Epoch {epoch} | Step {step} | Loss: {loss.item():.4f}\")\n\n    avg_train_loss = total_loss / max(1, step)\n    print(f\"\\nEpoch {epoch} finished. Avg train loss: {avg_train_loss:.4f}\")\n\n    # Evaluate\n    val_sim = evaluate_model(model, val_loader, processor, device)\n    print(f\"Epoch {epoch} | Validation mean_norm_levenshtein: {val_sim:.4f}\")\n\n    # Save best\n    if val_sim > best_val_sim:\n        best_val_sim = val_sim\n        print(f\"New best model! Saving to {best_model_path}\")\n        model.save_pretrained(best_model_path)\n        processor.save_pretrained(best_model_path)\n\nprint(f\"\\nTraining done. Best val similarity: {best_val_sim:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:58:35.395431Z","iopub.execute_input":"2025-11-16T18:58:35.396261Z","iopub.status.idle":"2025-11-16T18:58:38.464100Z","shell.execute_reply.started":"2025-11-16T18:58:35.396233Z","shell.execute_reply":"2025-11-16T18:58:38.463121Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/26197151.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             has_complex = self._init_group(\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 )\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 state[\"exp_avg_sq\"] = torch.zeros_like(\n\u001b[0m\u001b[1;32m    176\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 )\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 3274 has 14.73 GiB memory in use. Of the allocated memory 13.75 GiB is allocated by PyTorch, and 863.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 3274 has 14.73 GiB memory in use. Of the allocated memory 13.75 GiB is allocated by PyTorch, and 863.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":39},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration, WhisperProcessor\n\nfinetuned_processor = WhisperProcessor.from_pretrained(best_model_path)\nfinetuned_model = WhisperForConditionalGeneration.from_pretrained(best_model_path).to(device)\n\nfinetuned_model.config.forced_decoder_ids = finetuned_processor.get_decoder_prompt_ids(\n    language=\"bengali\",\n    task=\"transcribe\"\n)\nfinetuned_model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:31:42.089094Z","iopub.execute_input":"2025-11-16T18:31:42.089708Z","iopub.status.idle":"2025-11-16T18:31:42.990303Z","shell.execute_reply.started":"2025-11-16T18:31:42.089655Z","shell.execute_reply":"2025-11-16T18:31:42.989725Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperEncoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n      (embed_positions): WhisperPositionalEmbedding(448, 768)\n      (layers): ModuleList(\n        (0-11): 12 x WhisperDecoderLayer(\n          (self_attn): WhisperAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"def transcribe_finetuned(audio_path: str) -> str:\n    audio_array, sr = librosa.load(audio_path, sr=16000)\n    inputs = finetuned_processor(\n        audio_array,\n        sampling_rate=16000,\n        return_tensors=\"pt\"\n    )\n    input_features = inputs.input_features.to(device)\n\n    with torch.no_grad():\n        generated_ids = finetuned_model.generate(\n            input_features,\n            max_length=128\n        )\n\n    pred = finetuned_processor.tokenizer.batch_decode(\n        generated_ids,\n        skip_special_tokens=True\n    )[0]\n\n    pred = normalize_bn(pred)\n    return pred\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:31:48.593783Z","iopub.execute_input":"2025-11-16T18:31:48.594429Z","iopub.status.idle":"2025-11-16T18:31:48.598953Z","shell.execute_reply.started":"2025-11-16T18:31:48.594406Z","shell.execute_reply":"2025-11-16T18:31:48.598252Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"test_root = DATA_ROOT / \"Test\"\ntest_files = sorted(test_root.glob(\"*.wav\"))\n\nprint(\"Number of test files:\", len(test_files))\nprint(\"First few:\", [p.name for p in test_files[:10]])\n\npred_rows = []\nfor wav_path in tqdm(test_files):\n    file_name = wav_path.name\n    pred_text = transcribe_finetuned(str(wav_path))\n    pred_rows.append({\n        \"audio\": file_name,\n        \"text\": pred_text\n    })\n\nsubmission_df = pd.DataFrame(pred_rows)\nsubmission_df = submission_df.sort_values(\"audio\")\nsubmission_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:31:52.666421Z","iopub.execute_input":"2025-11-16T18:31:52.667213Z","iopub.status.idle":"2025-11-16T18:39:17.090802Z","shell.execute_reply.started":"2025-11-16T18:31:52.667187Z","shell.execute_reply":"2025-11-16T18:39:17.090168Z"}},"outputs":[{"name":"stdout","text":"Number of test files: 450\nFirst few: ['test_001.wav', 'test_002.wav', 'test_003.wav', 'test_004.wav', 'test_005.wav', 'test_006.wav', 'test_007.wav', 'test_008.wav', 'test_009.wav', 'test_010.wav']\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/450 [00:00<?, ?it/s]`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\nA custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\nA custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n100%|██████████| 450/450 [07:24<00:00,  1.01it/s]\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"          audio                            text\n0  test_001.wav        তুমি কি আগামে গল্প করবে?\n1  test_002.wav       তুমি কি আমাকে কলমটা দেবে?\n2  test_003.wav  আজ দুপুরে রাস্তায় পানি জমেছে।\n3  test_004.wav   আজকের সকালে হাঁটা খেতে বসেছে।\n4  test_005.wav     তুমি কি মোবাইল করতে যাচ্ছো?","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test_001.wav</td>\n      <td>তুমি কি আগামে গল্প করবে?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test_002.wav</td>\n      <td>তুমি কি আমাকে কলমটা দেবে?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test_003.wav</td>\n      <td>আজ দুপুরে রাস্তায় পানি জমেছে।</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test_004.wav</td>\n      <td>আজকের সকালে হাঁটা খেতে বসেছে।</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test_005.wav</td>\n      <td>তুমি কি মোবাইল করতে যাচ্ছো?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"submission_path = Path(\"submission.csv\")\nsubmission_df.to_csv(submission_path, index=False, encoding=\"utf-8\")\nprint(\"Saved submission to:\", submission_path.resolve())\n\nsubmission_df.head(20)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:39:48.299216Z","iopub.execute_input":"2025-11-16T18:39:48.299824Z","iopub.status.idle":"2025-11-16T18:39:48.310759Z","shell.execute_reply.started":"2025-11-16T18:39:48.299798Z","shell.execute_reply":"2025-11-16T18:39:48.310021Z"}},"outputs":[{"name":"stdout","text":"Saved submission to: /kaggle/working/submission.csv\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"           audio                                       text\n0   test_001.wav                   তুমি কি আগামে গল্প করবে?\n1   test_002.wav                  তুমি কি আমাকে কলমটা দেবে?\n2   test_003.wav             আজ দুপুরে রাস্তায় পানি জমেছে।\n3   test_004.wav              আজকের সকালে হাঁটা খেতে বসেছে।\n4   test_005.wav                তুমি কি মোবাইল করতে যাচ্ছো?\n5   test_006.wav               তুমি কি মোবাইল করণ্ড হয়েছো?\n6   test_007.wav             আজকের দোকে মেঘ জম পরে ক্রিকেন।\n7   test_008.wav                আমার ছোট ভাই স্কুলে যাচ্ছে।\n8   test_009.wav             তুমি কি গান গাইলে গাইলে গাইলে?\n9   test_010.wav         তুমি কি আজ বন্ধুর সাথে দেখা করেছো?\n10  test_011.wav             দরজাটা ধীরে ধীরে বন্ধ করে দাও।\n11  test_012.wav                   আমি আজ নতুন জুতো কিনেছি।\n12  test_013.wav                     তুমি কি আজ অফিসে যাবে?\n13  test_014.wav             আমি সকালের নাস্তা হিম খেয়েছি।\n14  test_015.wav            আজ দুপুরে বসে রাস্তায় পড়েছিল।\n15  test_016.wav  আমি বিকেলে হাঁটা হাঁটা হাঁটা হাঁটা হাঁটা।\n16  test_017.wav           আমাদের স্কুলে আজ বিশ্রা উঠে উঠে।\n17  test_018.wav                  আমি বাজার থেকে আম কিনেছি।\n18  test_019.wav                 তুমি কি কখনো কবিতা লিখেছো?\n19  test_020.wav      আমার নতুন বন্ধুদে চার্জার করে গিমেছি।","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>test_001.wav</td>\n      <td>তুমি কি আগামে গল্প করবে?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>test_002.wav</td>\n      <td>তুমি কি আমাকে কলমটা দেবে?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>test_003.wav</td>\n      <td>আজ দুপুরে রাস্তায় পানি জমেছে।</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>test_004.wav</td>\n      <td>আজকের সকালে হাঁটা খেতে বসেছে।</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>test_005.wav</td>\n      <td>তুমি কি মোবাইল করতে যাচ্ছো?</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>test_006.wav</td>\n      <td>তুমি কি মোবাইল করণ্ড হয়েছো?</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>test_007.wav</td>\n      <td>আজকের দোকে মেঘ জম পরে ক্রিকেন।</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>test_008.wav</td>\n      <td>আমার ছোট ভাই স্কুলে যাচ্ছে।</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>test_009.wav</td>\n      <td>তুমি কি গান গাইলে গাইলে গাইলে?</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>test_010.wav</td>\n      <td>তুমি কি আজ বন্ধুর সাথে দেখা করেছো?</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>test_011.wav</td>\n      <td>দরজাটা ধীরে ধীরে বন্ধ করে দাও।</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>test_012.wav</td>\n      <td>আমি আজ নতুন জুতো কিনেছি।</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>test_013.wav</td>\n      <td>তুমি কি আজ অফিসে যাবে?</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>test_014.wav</td>\n      <td>আমি সকালের নাস্তা হিম খেয়েছি।</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>test_015.wav</td>\n      <td>আজ দুপুরে বসে রাস্তায় পড়েছিল।</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>test_016.wav</td>\n      <td>আমি বিকেলে হাঁটা হাঁটা হাঁটা হাঁটা হাঁটা।</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>test_017.wav</td>\n      <td>আমাদের স্কুলে আজ বিশ্রা উঠে উঠে।</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>test_018.wav</td>\n      <td>আমি বাজার থেকে আম কিনেছি।</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>test_019.wav</td>\n      <td>তুমি কি কখনো কবিতা লিখেছো?</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>test_020.wav</td>\n      <td>আমার নতুন বন্ধুদে চার্জার করে গিমেছি।</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"import os\n\nprint(\"CWD:\", os.getcwd())\nprint(\"Files here:\", os.listdir())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T18:39:52.771214Z","iopub.execute_input":"2025-11-16T18:39:52.771481Z","iopub.status.idle":"2025-11-16T18:39:52.776189Z","shell.execute_reply.started":"2025-11-16T18:39:52.771459Z","shell.execute_reply":"2025-11-16T18:39:52.775421Z"}},"outputs":[{"name":"stdout","text":"CWD: /kaggle/working\nFiles here: ['whisper-small-shobdotori-manual', '.virtual_documents', 'submission.csv']\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!ls -al\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T10:44:15.508449Z","iopub.execute_input":"2025-11-16T10:44:15.509059Z","iopub.status.idle":"2025-11-16T10:44:15.712600Z","shell.execute_reply.started":"2025-11-16T10:44:15.509033Z","shell.execute_reply":"2025-11-16T10:44:15.711884Z"}},"outputs":[{"name":"stdout","text":"total 56\ndrwxr-xr-x 4 root root  4096 Nov 16 10:28 .\ndrwxr-xr-x 5 root root  4096 Nov 16 05:32 ..\n-rw-r--r-- 1 root root 40596 Nov 16 10:37 submission.csv\ndrwxr-xr-x 2 root root  4096 Nov 16 05:32 .virtual_documents\ndrwxr-xr-x 2 root root  4096 Nov 16 06:20 whisper-small-shobdotori-manual\n","output_type":"stream"}],"execution_count":39}]}